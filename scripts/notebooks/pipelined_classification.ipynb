{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from transformers import (AutoModelForSequenceClassification,\n",
    "                          AutoTokenizer)\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_path = Path('../../../BRUM/data/cross_validation')\n",
    "humor_features_cv_path = Path('../../../BRUM/data/features/humor_features')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumorRecognitionModel(torch.nn.Module):\n",
    "    def __init__(self, checkpoint, num_labels):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.base_model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.linear = torch.nn.Linear(2 + 27, self.num_labels)\n",
    "\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, humor_features, labels=None, **kwargs):\n",
    "        seq_output = self.base_model(input_ids=input_ids,\n",
    "                                     attention_mask=attention_mask)\n",
    "        concat_repr = torch.cat((seq_output.logits, humor_features), dim=1)\n",
    "        dropout_output = self.dropout(concat_repr)\n",
    "        logits = self.linear(dropout_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss,\n",
    "                                        logits=logits,\n",
    "                                        hidden_states=seq_output.hidden_states,\n",
    "                                        attentions=seq_output.attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(data, checkpoint, batch_size=16):\n",
    "    dataset = datasets.Dataset.from_pandas(data[['Text', 'Label']].reset_index())\n",
    "    tokenized_data = dataset.rename_column('Label', 'label')\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "    def tokenize_function(data):\n",
    "        label_batch = [element['label'] for element in data]\n",
    "        index_batch = [element['index'] for element in data]\n",
    "        text_batch = [element['Text'] for element in data]\n",
    "        tokenized = tokenizer(text_batch,\n",
    "                              truncation=True,\n",
    "                              padding='longest',\n",
    "                              return_tensors='pt')\n",
    "        return tokenized, index_batch, label_batch\n",
    "\n",
    "    dataloader = DataLoader(tokenized_data,\n",
    "                            batch_size=batch_size,\n",
    "                            collate_fn=tokenize_function)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "learning_rate = 5e-4\n",
    "\n",
    "accelerator = Accelerator(mixed_precision='fp16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_knowledge_injection(train, checkpoint, features, output, freeze_downstream=True):\n",
    "    model = HumorRecognitionModel(checkpoint, 2)\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=learning_rate)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.LinearLR(optimizer)\n",
    "\n",
    "    label2id = model.base_model.config.label2id\n",
    "    train['Label'] = train['Label'].map(label2id)\n",
    "    dataloader = tokenize_data(train, checkpoint, batch_size=16)\n",
    "\n",
    "    model, optimizer, dataloader, lr_scheduler = accelerator.prepare(model, optimizer, dataloader, lr_scheduler)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        with tqdm(dataloader, unit='batch') as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f'Epoch {epoch}')\n",
    "\n",
    "                inputs = dict(batch[0])\n",
    "                batch_features = features.iloc[batch[1], :].values\n",
    "                inputs['humor_features'] = torch.Tensor(batch_features.tolist()).to(device)\n",
    "                inputs['labels'] = torch.Tensor(batch[2]).long().to(device)\n",
    "\n",
    "                outputs = model(**inputs)\n",
    "                loss = outputs.loss\n",
    "                accelerator.backward(loss)\n",
    "\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                tepoch.set_postfix(training_loss=loss.item())\n",
    "\n",
    "    output.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(model.state_dict(), output/'checkpoint.pt')\n",
    "\n",
    "    accelerator.free_memory()\n",
    "    del model, optimizer, dataloader, lr_scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ..\\..\\results\\models\\bertimbau\\fold_0\\checkpoint-3312\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"..\\\\..\\\\results\\\\models\\\\bertimbau\\\\fold_0\\\\checkpoint-3312\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"H\",\n",
      "    \"1\": \"N\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"H\": 0,\n",
      "    \"N\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.34.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 29794\n",
      "}\n",
      "\n",
      "loading weights file ..\\..\\results\\models\\bertimbau\\fold_0\\checkpoint-3312\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ..\\..\\results\\models\\bertimbau\\fold_0\\checkpoint-3312.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7c9d86fc4b47bf9711909c5bf459aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/552 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "205e88b76a5f4f74b48dad5e5d04a5ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/552 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7f9a9b955c479c81551dd58ddbc33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/552 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab7beed6697435d8ff70ad831d48aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/552 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a669e8c604048909ad8e95c1e20bcfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/552 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for fold in cv_path.iterdir():\n",
    "    train = pd.read_json(fold/'train.json')[['Text', 'Label']]\n",
    "    checkpoint = list((Path('../../results/models/bertimbau')/fold.name).iterdir())[0]\n",
    "    features = pd.read_hdf(humor_features_cv_path/fold.name/'train'/'data.hdf5')\n",
    "    features = features.drop(columns='Label')\n",
    "    output = Path('../../results/models/pipeline_bertimbau')/fold.name\n",
    "    finetune_knowledge_injection(train, checkpoint, features, output)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knowledge_injection_prediction(test, features, downstream_checkpoint, checkpoint, output):\n",
    "    # Load model\n",
    "    model = HumorRecognitionModel(downstream_checkpoint, 2).to(device)\n",
    "    loaded_model = torch.load(checkpoint)\n",
    "    model.load_state_dict(loaded_model)\n",
    "    model.eval()\n",
    "\n",
    "    # Convert labels to model ids\n",
    "    label2id = model.base_model.config.label2id\n",
    "    id2label = model.base_model.config.id2label\n",
    "    test['Label'] = test['Label'].map(label2id)\n",
    "    dataloader = tokenize_data(test, downstream_checkpoint, batch_size=64)\n",
    "\n",
    "    model, dataloader = accelerator.prepare(model, dataloader)\n",
    "\n",
    "    results_dict = {'Prediction': list(),\n",
    "                    'Label': list()}\n",
    "    for batch in tqdm(dataloader, unit='batch'):\n",
    "        # Compute model inputs\n",
    "        inputs = dict(batch[0])\n",
    "        batch_features = features.iloc[batch[1], :].values\n",
    "        inputs['humor_features'] = torch.Tensor(batch_features.tolist()).to(device)\n",
    "        inputs['labels'] = torch.Tensor(batch[2]).to(device).long()\n",
    "\n",
    "        # Do prediction\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits.softmax(1), 1)\n",
    "\n",
    "        # Create predictions dataframe\n",
    "        results_dict['Prediction'].extend(predictions.tolist())\n",
    "        results_dict['Label'].extend(inputs['labels'].tolist())\n",
    "    results = pd.DataFrame(results_dict, index=test.index)\n",
    "    results['Prediction'] = results['Prediction'].map(model.base_model.config.id2label)\n",
    "    results['Label'] = results['Label'].map(id2label)\n",
    "\n",
    "    # Save predictions\n",
    "    output.parent.mkdir(parents=True, exist_ok=True)\n",
    "    results.to_csv(output, encoding='utf-8')\n",
    "\n",
    "    accelerator.free_memory()\n",
    "    del model, dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ..\\..\\results\\models\\bertimbau\\fold_0\\checkpoint-3312\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"..\\\\..\\\\results\\\\models\\\\bertimbau\\\\fold_0\\\\checkpoint-3312\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"H\",\n",
      "    \"1\": \"N\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"H\": 0,\n",
      "    \"N\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.34.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 29794\n",
      "}\n",
      "\n",
      "loading weights file ..\\..\\results\\models\\bertimbau\\fold_0\\checkpoint-3312\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ..\\..\\results\\models\\bertimbau\\fold_0\\checkpoint-3312.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "975bcc6bd6224ba48a96e07c20bc363b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "for fold in cv_path.iterdir():\n",
    "    test = pd.read_json(fold/'test.json')[['Text', 'Label']]\n",
    "\n",
    "    features = pd.read_hdf(humor_features_cv_path/fold.name/'train'/'data.hdf5')\n",
    "    features = features.drop(columns='Label')\n",
    "    \n",
    "    downstream_checkpoint_folder = Path('../../results/models/bertimbau')/fold.name\n",
    "    downstream_checkpoint = list(downstream_checkpoint_folder.iterdir())[0]\n",
    "\n",
    "    checkpoint_folder = Path('../../results/models/pipeline_bertimbau')\n",
    "    checkpoint = checkpoint_folder/fold.name/'checkpoint.pt'\n",
    "\n",
    "    output_folder = Path('../../results/predictions/pipeline_bertimbau')/fold.name\n",
    "    output = output_folder.with_suffix('.csv')\n",
    "\n",
    "    knowledge_injection_prediction(test, features, downstream_checkpoint, checkpoint, output)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>H</th>\n",
       "      <th>N</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Method</th>\n",
       "      <th>Fold</th>\n",
       "      <th>Metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">albertina_ptbr_base</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">fold_0</th>\n",
       "      <th>precision</th>\n",
       "      <td>0.740576</td>\n",
       "      <td>0.703774</td>\n",
       "      <td>0.720693</td>\n",
       "      <td>0.722175</td>\n",
       "      <td>0.722194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.680244</td>\n",
       "      <td>0.761224</td>\n",
       "      <td>0.720693</td>\n",
       "      <td>0.720734</td>\n",
       "      <td>0.720693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.709130</td>\n",
       "      <td>0.731373</td>\n",
       "      <td>0.720693</td>\n",
       "      <td>0.720251</td>\n",
       "      <td>0.720240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>491.000000</td>\n",
       "      <td>490.000000</td>\n",
       "      <td>0.720693</td>\n",
       "      <td>981.000000</td>\n",
       "      <td>981.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold_1</th>\n",
       "      <th>precision</th>\n",
       "      <td>0.797531</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.745158</td>\n",
       "      <td>0.752932</td>\n",
       "      <td>0.752978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knwoledge_injection_bertimbau_frozen</th>\n",
       "      <th>fold_9</th>\n",
       "      <th>support</th>\n",
       "      <td>490.000000</td>\n",
       "      <td>490.000000</td>\n",
       "      <td>0.708163</td>\n",
       "      <td>980.000000</td>\n",
       "      <td>980.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">pipeline_bertimbau</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">fold_0</th>\n",
       "      <th>precision</th>\n",
       "      <td>0.693182</td>\n",
       "      <td>0.656192</td>\n",
       "      <td>0.672783</td>\n",
       "      <td>0.674687</td>\n",
       "      <td>0.674706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.621181</td>\n",
       "      <td>0.724490</td>\n",
       "      <td>0.672783</td>\n",
       "      <td>0.672836</td>\n",
       "      <td>0.672783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.655209</td>\n",
       "      <td>0.688652</td>\n",
       "      <td>0.672783</td>\n",
       "      <td>0.671931</td>\n",
       "      <td>0.671914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>491.000000</td>\n",
       "      <td>490.000000</td>\n",
       "      <td>0.672783</td>\n",
       "      <td>981.000000</td>\n",
       "      <td>981.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                H           N  \\\n",
       "Method                               Fold   Metric                              \n",
       "albertina_ptbr_base                  fold_0 precision    0.740576    0.703774   \n",
       "                                            recall       0.680244    0.761224   \n",
       "                                            f1-score     0.709130    0.731373   \n",
       "                                            support    491.000000  490.000000   \n",
       "                                     fold_1 precision    0.797531    0.708333   \n",
       "...                                                           ...         ...   \n",
       "knwoledge_injection_bertimbau_frozen fold_9 support    490.000000  490.000000   \n",
       "pipeline_bertimbau                   fold_0 precision    0.693182    0.656192   \n",
       "                                            recall       0.621181    0.724490   \n",
       "                                            f1-score     0.655209    0.688652   \n",
       "                                            support    491.000000  490.000000   \n",
       "\n",
       "                                                       accuracy   macro avg  \\\n",
       "Method                               Fold   Metric                            \n",
       "albertina_ptbr_base                  fold_0 precision  0.720693    0.722175   \n",
       "                                            recall     0.720693    0.720734   \n",
       "                                            f1-score   0.720693    0.720251   \n",
       "                                            support    0.720693  981.000000   \n",
       "                                     fold_1 precision  0.745158    0.752932   \n",
       "...                                                         ...         ...   \n",
       "knwoledge_injection_bertimbau_frozen fold_9 support    0.708163  980.000000   \n",
       "pipeline_bertimbau                   fold_0 precision  0.672783    0.674687   \n",
       "                                            recall     0.672783    0.672836   \n",
       "                                            f1-score   0.672783    0.671931   \n",
       "                                            support    0.672783  981.000000   \n",
       "\n",
       "                                                       weighted avg  \n",
       "Method                               Fold   Metric                   \n",
       "albertina_ptbr_base                  fold_0 precision      0.722194  \n",
       "                                            recall         0.720693  \n",
       "                                            f1-score       0.720240  \n",
       "                                            support      981.000000  \n",
       "                                     fold_1 precision      0.752978  \n",
       "...                                                             ...  \n",
       "knwoledge_injection_bertimbau_frozen fold_9 support      980.000000  \n",
       "pipeline_bertimbau                   fold_0 precision      0.674706  \n",
       "                                            recall         0.672783  \n",
       "                                            f1-score       0.671914  \n",
       "                                            support      981.000000  \n",
       "\n",
       "[284 rows x 5 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_path = Path('../../results/predictions')\n",
    "\n",
    "results = dict()\n",
    "for method in predictions_path.iterdir():\n",
    "    folds = dict()\n",
    "    for fold in method.iterdir():\n",
    "        fold_df = pd.read_csv(fold)\n",
    "        evaluation = classification_report(fold_df['Label'],\n",
    "                                           fold_df['Prediction'],\n",
    "                                           output_dict=True)\n",
    "        evaluation_df = pd.DataFrame.from_dict(evaluation)\n",
    "        folds[fold.stem] = evaluation_df\n",
    "    results[method.stem] = pd.concat(folds, names=['Fold'])\n",
    "results_df = pd.concat(results, names=['Method'])\n",
    "results_df.index = results_df.index.rename('Metric', level=2)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>H</th>\n",
       "      <th>N</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Method</th>\n",
       "      <th>Fold</th>\n",
       "      <th>Metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">albertina_ptbr_base</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">fold_0</th>\n",
       "      <th>precision</th>\n",
       "      <td>0.740576</td>\n",
       "      <td>0.703774</td>\n",
       "      <td>0.720693</td>\n",
       "      <td>0.722175</td>\n",
       "      <td>0.722194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.680244</td>\n",
       "      <td>0.761224</td>\n",
       "      <td>0.720693</td>\n",
       "      <td>0.720734</td>\n",
       "      <td>0.720693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.709130</td>\n",
       "      <td>0.731373</td>\n",
       "      <td>0.720693</td>\n",
       "      <td>0.720251</td>\n",
       "      <td>0.720240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>491.000000</td>\n",
       "      <td>490.000000</td>\n",
       "      <td>0.720693</td>\n",
       "      <td>981.000000</td>\n",
       "      <td>981.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">albertina_ptpt_base</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">fold_0</th>\n",
       "      <th>precision</th>\n",
       "      <td>0.737527</td>\n",
       "      <td>0.709615</td>\n",
       "      <td>0.722732</td>\n",
       "      <td>0.723571</td>\n",
       "      <td>0.723585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.692464</td>\n",
       "      <td>0.753061</td>\n",
       "      <td>0.722732</td>\n",
       "      <td>0.722763</td>\n",
       "      <td>0.722732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.730693</td>\n",
       "      <td>0.722732</td>\n",
       "      <td>0.722489</td>\n",
       "      <td>0.722481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>491.000000</td>\n",
       "      <td>490.000000</td>\n",
       "      <td>0.722732</td>\n",
       "      <td>981.000000</td>\n",
       "      <td>981.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">bertimbau</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">fold_0</th>\n",
       "      <th>precision</th>\n",
       "      <td>0.693878</td>\n",
       "      <td>0.657407</td>\n",
       "      <td>0.673802</td>\n",
       "      <td>0.675642</td>\n",
       "      <td>0.675661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.623218</td>\n",
       "      <td>0.724490</td>\n",
       "      <td>0.673802</td>\n",
       "      <td>0.673854</td>\n",
       "      <td>0.673802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.656652</td>\n",
       "      <td>0.689320</td>\n",
       "      <td>0.673802</td>\n",
       "      <td>0.672986</td>\n",
       "      <td>0.672970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>491.000000</td>\n",
       "      <td>490.000000</td>\n",
       "      <td>0.673802</td>\n",
       "      <td>981.000000</td>\n",
       "      <td>981.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">knowledge_injection_albertina_ptbr_base_frozen</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">fold_0</th>\n",
       "      <th>precision</th>\n",
       "      <td>0.764019</td>\n",
       "      <td>0.703436</td>\n",
       "      <td>0.729867</td>\n",
       "      <td>0.733727</td>\n",
       "      <td>0.733758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.665988</td>\n",
       "      <td>0.793878</td>\n",
       "      <td>0.729867</td>\n",
       "      <td>0.729933</td>\n",
       "      <td>0.729867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.711643</td>\n",
       "      <td>0.745925</td>\n",
       "      <td>0.729867</td>\n",
       "      <td>0.728784</td>\n",
       "      <td>0.728767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>491.000000</td>\n",
       "      <td>490.000000</td>\n",
       "      <td>0.729867</td>\n",
       "      <td>981.000000</td>\n",
       "      <td>981.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">knowledge_injection_albertina_ptpt_base_frozen</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">fold_0</th>\n",
       "      <th>precision</th>\n",
       "      <td>0.734649</td>\n",
       "      <td>0.702857</td>\n",
       "      <td>0.717635</td>\n",
       "      <td>0.718753</td>\n",
       "      <td>0.718769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.682281</td>\n",
       "      <td>0.753061</td>\n",
       "      <td>0.717635</td>\n",
       "      <td>0.717671</td>\n",
       "      <td>0.717635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.707497</td>\n",
       "      <td>0.727094</td>\n",
       "      <td>0.717635</td>\n",
       "      <td>0.717295</td>\n",
       "      <td>0.717285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>491.000000</td>\n",
       "      <td>490.000000</td>\n",
       "      <td>0.717635</td>\n",
       "      <td>981.000000</td>\n",
       "      <td>981.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">knowledge_injection_bertimbau_not_frozen</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">fold_0</th>\n",
       "      <th>precision</th>\n",
       "      <td>0.651961</td>\n",
       "      <td>0.607330</td>\n",
       "      <td>0.625892</td>\n",
       "      <td>0.629645</td>\n",
       "      <td>0.629668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.541752</td>\n",
       "      <td>0.710204</td>\n",
       "      <td>0.625892</td>\n",
       "      <td>0.625978</td>\n",
       "      <td>0.625892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.591769</td>\n",
       "      <td>0.654751</td>\n",
       "      <td>0.625892</td>\n",
       "      <td>0.623260</td>\n",
       "      <td>0.623228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>491.000000</td>\n",
       "      <td>490.000000</td>\n",
       "      <td>0.625892</td>\n",
       "      <td>981.000000</td>\n",
       "      <td>981.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">knwoledge_injection_bertimbau_frozen</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">fold_0</th>\n",
       "      <th>precision</th>\n",
       "      <td>0.688172</td>\n",
       "      <td>0.668605</td>\n",
       "      <td>0.677880</td>\n",
       "      <td>0.678388</td>\n",
       "      <td>0.678398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.651731</td>\n",
       "      <td>0.704082</td>\n",
       "      <td>0.677880</td>\n",
       "      <td>0.677906</td>\n",
       "      <td>0.677880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.669456</td>\n",
       "      <td>0.685885</td>\n",
       "      <td>0.677880</td>\n",
       "      <td>0.677670</td>\n",
       "      <td>0.677662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>491.000000</td>\n",
       "      <td>490.000000</td>\n",
       "      <td>0.677880</td>\n",
       "      <td>981.000000</td>\n",
       "      <td>981.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">pipeline_bertimbau</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">fold_0</th>\n",
       "      <th>precision</th>\n",
       "      <td>0.693182</td>\n",
       "      <td>0.656192</td>\n",
       "      <td>0.672783</td>\n",
       "      <td>0.674687</td>\n",
       "      <td>0.674706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.621181</td>\n",
       "      <td>0.724490</td>\n",
       "      <td>0.672783</td>\n",
       "      <td>0.672836</td>\n",
       "      <td>0.672783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.655209</td>\n",
       "      <td>0.688652</td>\n",
       "      <td>0.672783</td>\n",
       "      <td>0.671931</td>\n",
       "      <td>0.671914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>491.000000</td>\n",
       "      <td>490.000000</td>\n",
       "      <td>0.672783</td>\n",
       "      <td>981.000000</td>\n",
       "      <td>981.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          H  \\\n",
       "Method                                         Fold   Metric                  \n",
       "albertina_ptbr_base                            fold_0 precision    0.740576   \n",
       "                                                      recall       0.680244   \n",
       "                                                      f1-score     0.709130   \n",
       "                                                      support    491.000000   \n",
       "albertina_ptpt_base                            fold_0 precision    0.737527   \n",
       "                                                      recall       0.692464   \n",
       "                                                      f1-score     0.714286   \n",
       "                                                      support    491.000000   \n",
       "bertimbau                                      fold_0 precision    0.693878   \n",
       "                                                      recall       0.623218   \n",
       "                                                      f1-score     0.656652   \n",
       "                                                      support    491.000000   \n",
       "knowledge_injection_albertina_ptbr_base_frozen fold_0 precision    0.764019   \n",
       "                                                      recall       0.665988   \n",
       "                                                      f1-score     0.711643   \n",
       "                                                      support    491.000000   \n",
       "knowledge_injection_albertina_ptpt_base_frozen fold_0 precision    0.734649   \n",
       "                                                      recall       0.682281   \n",
       "                                                      f1-score     0.707497   \n",
       "                                                      support    491.000000   \n",
       "knowledge_injection_bertimbau_not_frozen       fold_0 precision    0.651961   \n",
       "                                                      recall       0.541752   \n",
       "                                                      f1-score     0.591769   \n",
       "                                                      support    491.000000   \n",
       "knwoledge_injection_bertimbau_frozen           fold_0 precision    0.688172   \n",
       "                                                      recall       0.651731   \n",
       "                                                      f1-score     0.669456   \n",
       "                                                      support    491.000000   \n",
       "pipeline_bertimbau                             fold_0 precision    0.693182   \n",
       "                                                      recall       0.621181   \n",
       "                                                      f1-score     0.655209   \n",
       "                                                      support    491.000000   \n",
       "\n",
       "                                                                          N  \\\n",
       "Method                                         Fold   Metric                  \n",
       "albertina_ptbr_base                            fold_0 precision    0.703774   \n",
       "                                                      recall       0.761224   \n",
       "                                                      f1-score     0.731373   \n",
       "                                                      support    490.000000   \n",
       "albertina_ptpt_base                            fold_0 precision    0.709615   \n",
       "                                                      recall       0.753061   \n",
       "                                                      f1-score     0.730693   \n",
       "                                                      support    490.000000   \n",
       "bertimbau                                      fold_0 precision    0.657407   \n",
       "                                                      recall       0.724490   \n",
       "                                                      f1-score     0.689320   \n",
       "                                                      support    490.000000   \n",
       "knowledge_injection_albertina_ptbr_base_frozen fold_0 precision    0.703436   \n",
       "                                                      recall       0.793878   \n",
       "                                                      f1-score     0.745925   \n",
       "                                                      support    490.000000   \n",
       "knowledge_injection_albertina_ptpt_base_frozen fold_0 precision    0.702857   \n",
       "                                                      recall       0.753061   \n",
       "                                                      f1-score     0.727094   \n",
       "                                                      support    490.000000   \n",
       "knowledge_injection_bertimbau_not_frozen       fold_0 precision    0.607330   \n",
       "                                                      recall       0.710204   \n",
       "                                                      f1-score     0.654751   \n",
       "                                                      support    490.000000   \n",
       "knwoledge_injection_bertimbau_frozen           fold_0 precision    0.668605   \n",
       "                                                      recall       0.704082   \n",
       "                                                      f1-score     0.685885   \n",
       "                                                      support    490.000000   \n",
       "pipeline_bertimbau                             fold_0 precision    0.656192   \n",
       "                                                      recall       0.724490   \n",
       "                                                      f1-score     0.688652   \n",
       "                                                      support    490.000000   \n",
       "\n",
       "                                                                 accuracy  \\\n",
       "Method                                         Fold   Metric                \n",
       "albertina_ptbr_base                            fold_0 precision  0.720693   \n",
       "                                                      recall     0.720693   \n",
       "                                                      f1-score   0.720693   \n",
       "                                                      support    0.720693   \n",
       "albertina_ptpt_base                            fold_0 precision  0.722732   \n",
       "                                                      recall     0.722732   \n",
       "                                                      f1-score   0.722732   \n",
       "                                                      support    0.722732   \n",
       "bertimbau                                      fold_0 precision  0.673802   \n",
       "                                                      recall     0.673802   \n",
       "                                                      f1-score   0.673802   \n",
       "                                                      support    0.673802   \n",
       "knowledge_injection_albertina_ptbr_base_frozen fold_0 precision  0.729867   \n",
       "                                                      recall     0.729867   \n",
       "                                                      f1-score   0.729867   \n",
       "                                                      support    0.729867   \n",
       "knowledge_injection_albertina_ptpt_base_frozen fold_0 precision  0.717635   \n",
       "                                                      recall     0.717635   \n",
       "                                                      f1-score   0.717635   \n",
       "                                                      support    0.717635   \n",
       "knowledge_injection_bertimbau_not_frozen       fold_0 precision  0.625892   \n",
       "                                                      recall     0.625892   \n",
       "                                                      f1-score   0.625892   \n",
       "                                                      support    0.625892   \n",
       "knwoledge_injection_bertimbau_frozen           fold_0 precision  0.677880   \n",
       "                                                      recall     0.677880   \n",
       "                                                      f1-score   0.677880   \n",
       "                                                      support    0.677880   \n",
       "pipeline_bertimbau                             fold_0 precision  0.672783   \n",
       "                                                      recall     0.672783   \n",
       "                                                      f1-score   0.672783   \n",
       "                                                      support    0.672783   \n",
       "\n",
       "                                                                  macro avg  \\\n",
       "Method                                         Fold   Metric                  \n",
       "albertina_ptbr_base                            fold_0 precision    0.722175   \n",
       "                                                      recall       0.720734   \n",
       "                                                      f1-score     0.720251   \n",
       "                                                      support    981.000000   \n",
       "albertina_ptpt_base                            fold_0 precision    0.723571   \n",
       "                                                      recall       0.722763   \n",
       "                                                      f1-score     0.722489   \n",
       "                                                      support    981.000000   \n",
       "bertimbau                                      fold_0 precision    0.675642   \n",
       "                                                      recall       0.673854   \n",
       "                                                      f1-score     0.672986   \n",
       "                                                      support    981.000000   \n",
       "knowledge_injection_albertina_ptbr_base_frozen fold_0 precision    0.733727   \n",
       "                                                      recall       0.729933   \n",
       "                                                      f1-score     0.728784   \n",
       "                                                      support    981.000000   \n",
       "knowledge_injection_albertina_ptpt_base_frozen fold_0 precision    0.718753   \n",
       "                                                      recall       0.717671   \n",
       "                                                      f1-score     0.717295   \n",
       "                                                      support    981.000000   \n",
       "knowledge_injection_bertimbau_not_frozen       fold_0 precision    0.629645   \n",
       "                                                      recall       0.625978   \n",
       "                                                      f1-score     0.623260   \n",
       "                                                      support    981.000000   \n",
       "knwoledge_injection_bertimbau_frozen           fold_0 precision    0.678388   \n",
       "                                                      recall       0.677906   \n",
       "                                                      f1-score     0.677670   \n",
       "                                                      support    981.000000   \n",
       "pipeline_bertimbau                             fold_0 precision    0.674687   \n",
       "                                                      recall       0.672836   \n",
       "                                                      f1-score     0.671931   \n",
       "                                                      support    981.000000   \n",
       "\n",
       "                                                                 weighted avg  \n",
       "Method                                         Fold   Metric                   \n",
       "albertina_ptbr_base                            fold_0 precision      0.722194  \n",
       "                                                      recall         0.720693  \n",
       "                                                      f1-score       0.720240  \n",
       "                                                      support      981.000000  \n",
       "albertina_ptpt_base                            fold_0 precision      0.723585  \n",
       "                                                      recall         0.722732  \n",
       "                                                      f1-score       0.722481  \n",
       "                                                      support      981.000000  \n",
       "bertimbau                                      fold_0 precision      0.675661  \n",
       "                                                      recall         0.673802  \n",
       "                                                      f1-score       0.672970  \n",
       "                                                      support      981.000000  \n",
       "knowledge_injection_albertina_ptbr_base_frozen fold_0 precision      0.733758  \n",
       "                                                      recall         0.729867  \n",
       "                                                      f1-score       0.728767  \n",
       "                                                      support      981.000000  \n",
       "knowledge_injection_albertina_ptpt_base_frozen fold_0 precision      0.718769  \n",
       "                                                      recall         0.717635  \n",
       "                                                      f1-score       0.717285  \n",
       "                                                      support      981.000000  \n",
       "knowledge_injection_bertimbau_not_frozen       fold_0 precision      0.629668  \n",
       "                                                      recall         0.625892  \n",
       "                                                      f1-score       0.623228  \n",
       "                                                      support      981.000000  \n",
       "knwoledge_injection_bertimbau_frozen           fold_0 precision      0.678398  \n",
       "                                                      recall         0.677880  \n",
       "                                                      f1-score       0.677662  \n",
       "                                                      support      981.000000  \n",
       "pipeline_bertimbau                             fold_0 precision      0.674706  \n",
       "                                                      recall         0.672783  \n",
       "                                                      f1-score       0.671914  \n",
       "                                                      support      981.000000  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.query('Fold == \"fold_0\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "humor-recognition-knowledge-injection-zyMLYgwE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
