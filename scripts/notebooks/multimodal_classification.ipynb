{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Humor Classification using Multimodal Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from humor_recognition.models import *\n",
    "from humor_recognition.tasks import train, predict\n",
    "\n",
    "from datasets import Dataset\n",
    "from pathlib import Path\n",
    "from transformers import (AutoModelForSequenceClassification,\n",
    "                          AutoTokenizer,\n",
    "                          DataCollatorWithPadding,\n",
    "                          Trainer,\n",
    "                          TrainingArguments,\n",
    "                          pipeline)\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus_path = Path('../../../BRUM/data/cross_validation/fold_0/train.json')\n",
    "train_features_path = '../../data/humor_features/fold_0/train.csv'\n",
    "test_corpus_path = Path('../../../BRUM/data/cross_validation/fold_0/test.json')\n",
    "test_features_path = Path('../../data/humor_features/fold_0/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bertimbau Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'neuralmind/bert-base-portuguese-cased'\n",
    "checkpoint_type = 'bert'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base model: $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1142d4c9f0c426e8c3462ba117764c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8825 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5407807026e45bb916f0317a7108f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5520 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6195, 'learning_rate': 4.547101449275363e-05, 'epoch': 0.45}\n",
      "{'loss': 0.5876, 'learning_rate': 4.094202898550725e-05, 'epoch': 0.91}\n",
      "{'loss': 0.478, 'learning_rate': 3.641304347826087e-05, 'epoch': 1.36}\n",
      "{'loss': 0.4663, 'learning_rate': 3.188405797101449e-05, 'epoch': 1.81}\n",
      "{'loss': 0.4057, 'learning_rate': 2.7355072463768118e-05, 'epoch': 2.26}\n",
      "{'loss': 0.3578, 'learning_rate': 2.282608695652174e-05, 'epoch': 2.72}\n",
      "{'loss': 0.3342, 'learning_rate': 1.8297101449275363e-05, 'epoch': 3.17}\n",
      "{'loss': 0.2873, 'learning_rate': 1.3768115942028985e-05, 'epoch': 3.62}\n",
      "{'loss': 0.2902, 'learning_rate': 9.239130434782608e-06, 'epoch': 4.08}\n",
      "{'loss': 0.2208, 'learning_rate': 4.710144927536232e-06, 'epoch': 4.53}\n",
      "{'loss': 0.2204, 'learning_rate': 1.8115942028985507e-07, 'epoch': 4.98}\n",
      "{'train_runtime': 457.7651, 'train_samples_per_second': 96.392, 'train_steps_per_second': 12.059, 'train_loss': 0.387457694782727, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "train_output = Path('results/models/bertimbau/fold_0')\n",
    "test_output = Path('results/predictions/bertimbau/fold_0.csv')\n",
    "\n",
    "###### TRAINING ######\n",
    "train_corpus = pd.read_json(train_corpus_path)\n",
    "labels = train_corpus['Label'].unique()\n",
    "num_labels = len(labels)\n",
    "label2id = {label: id_ for id_, label in enumerate(labels)}\n",
    "id2label = {label2id[label]: label for label in label2id}\n",
    "train_corpus['Label'] = train_corpus['Label'].map(label2id)\n",
    "\n",
    "data = Dataset.from_pandas(train_corpus[['Text', 'Label']])\n",
    "data = data.rename_column('Label', 'label')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['Text'])\n",
    "\n",
    "\n",
    "tokenized_data = data.map(tokenize_function, batched=True)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint,\n",
    "                                                           num_labels=num_labels,\n",
    "                                                           label2id=label2id,\n",
    "                                                           id2label=id2label)\n",
    "\n",
    "training_args = TrainingArguments(train_output,\n",
    "                                  save_strategy='epoch',\n",
    "                                  save_total_limit=1,\n",
    "                                  learning_rate=5e-5,\n",
    "                                  num_train_epochs=5)\n",
    "trainer = Trainer(model, training_args,\n",
    "                  train_dataset=tokenized_data,\n",
    "                  data_collator=data_collator,\n",
    "                  tokenizer=tokenizer)\n",
    "trainer.train()\n",
    "\n",
    "###### PRECITION ######\n",
    "test_corpus = pd.read_json(test_corpus_path)\n",
    "test_data = test_corpus['Text'].to_list()\n",
    "\n",
    "text_classification = pipeline('text-classification',\n",
    "                               model=model,\n",
    "                               tokenizer=tokenizer,\n",
    "                               device=0)\n",
    "predictions = text_classification(test_data)\n",
    "\n",
    "results = pd.DataFrame(predictions, index=test_corpus.index)\n",
    "results = results.drop(columns='score')\n",
    "results = results.rename(columns={'label': 'Prediction'})\n",
    "results['Label'] = test_corpus['Label']\n",
    "\n",
    "test_output.parent.mkdir(parents=True, exist_ok=True)\n",
    "results.to_csv(test_output, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation: $x\\|n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4defcbe53b37494f90abff1166cd75eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5917, 'learning_rate': 4.094202898550725e-05, 'epoch': 0.91}\n",
      "{'loss': 0.4472, 'learning_rate': 3.188405797101449e-05, 'epoch': 1.81}\n",
      "{'loss': 0.3392, 'learning_rate': 2.282608695652174e-05, 'epoch': 2.72}\n",
      "{'loss': 0.2661, 'learning_rate': 1.3768115942028985e-05, 'epoch': 3.62}\n",
      "{'loss': 0.2177, 'learning_rate': 4.710144927536232e-06, 'epoch': 4.53}\n",
      "{'train_runtime': 415.9902, 'train_samples_per_second': 106.072, 'train_steps_per_second': 6.635, 'train_loss': 0.35486069278440613, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "train_output = Path('results/models/bertimbau_concatenation/fold_0')\n",
    "test_output = Path('results/predictions/bertimbau_concatenation/fold_0.csv')\n",
    "\n",
    "trained_model = train(train_corpus_path,\n",
    "                      train_features_path,\n",
    "                      checkpoint,\n",
    "                      checkpoint_type,\n",
    "                      ClassificationModelConcatenation,\n",
    "                      train_output)\n",
    "predict(test_corpus_path,\n",
    "        test_features_path,\n",
    "        trained_model,\n",
    "        test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features pooling: $x\\|\\mathrm{MLP}(n)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c4e469585f444da45f484b1cdad4a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5898, 'learning_rate': 4.094202898550725e-05, 'epoch': 0.91}\n",
      "{'loss': 0.4449, 'learning_rate': 3.188405797101449e-05, 'epoch': 1.81}\n",
      "{'loss': 0.3316, 'learning_rate': 2.282608695652174e-05, 'epoch': 2.72}\n",
      "{'loss': 0.2714, 'learning_rate': 1.3768115942028985e-05, 'epoch': 3.62}\n",
      "{'loss': 0.2067, 'learning_rate': 4.710144927536232e-06, 'epoch': 4.53}\n",
      "{'train_runtime': 416.9316, 'train_samples_per_second': 105.833, 'train_steps_per_second': 6.62, 'train_loss': 0.35074825286865235, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "train_output = Path('results/models/bertimbau_features_pooling/fold_0')\n",
    "test_output = Path('results/predictions/bertimbau_features_pooling/fold_0.csv')\n",
    "\n",
    "trained_model = train(train_corpus_path,\n",
    "                      train_features_path,\n",
    "                      checkpoint,\n",
    "                      checkpoint_type,\n",
    "                      ClassificationModelFeaturesPooling,\n",
    "                      train_output)\n",
    "predict(test_corpus_path,\n",
    "        test_features_path,\n",
    "        trained_model,\n",
    "        test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared representation: $\\mathrm{MLP}(x\\|\\mathrm{MLP(n)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db52d0ee27ce474b8be5196e3fdca0c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5914, 'learning_rate': 4.094202898550725e-05, 'epoch': 0.91}\n",
      "{'loss': 0.446, 'learning_rate': 3.188405797101449e-05, 'epoch': 1.81}\n",
      "{'loss': 0.3407, 'learning_rate': 2.282608695652174e-05, 'epoch': 2.72}\n",
      "{'loss': 0.2807, 'learning_rate': 1.3768115942028985e-05, 'epoch': 3.62}\n",
      "{'loss': 0.2283, 'learning_rate': 4.710144927536232e-06, 'epoch': 4.53}\n",
      "{'train_runtime': 408.4961, 'train_samples_per_second': 108.018, 'train_steps_per_second': 6.756, 'train_loss': 0.36211222358371903, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "train_output = Path('results/models/bertimbau_shared_representation/fold_0')\n",
    "test_output = Path('results/predictions/bertimbau_shared_representation/fold_0.csv')\n",
    "\n",
    "trained_model = train(train_corpus_path,\n",
    "                      train_features_path,\n",
    "                      checkpoint,\n",
    "                      checkpoint_type,\n",
    "                      ClassificationModelSharedRepresentation,\n",
    "                      train_output)\n",
    "predict(test_corpus_path,\n",
    "        test_features_path,\n",
    "        trained_model,\n",
    "        test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bertimbau Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'neuralmind/bert-large-portuguese-cased'\n",
    "checkpoint_type = 'bert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69179c4e76a04ae5b6cb71586a82706f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/155 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\.virtualenvs\\humor-recognition-knowledge-injection-zyMLYgwE\\lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Márcio Lima\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c138186c0ff42ad8b7bf3500c7f2b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/648 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a835701e58074ad5883cf86c4b5e508b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/210k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b456ab7c9d7a4a6db55c03aa949bdecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)in/added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4a53f2865b4f239fb62bd80ee02adb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "604f2f68e0b1443fa2e80f65def42140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8825 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a78f870a3f94bea9dd6749a1055fbc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-large-portuguese-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fad1c7469444681983bb85e38bc299c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5520 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Documentos\\Doutorado\\Projeto\\Criatividade\\Projetos\\humor-recognition-knowledge-injection\\scripts\\notebooks\\multimodal_classification.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documentos/Doutorado/Projeto/Criatividade/Projetos/humor-recognition-knowledge-injection/scripts/notebooks/multimodal_classification.ipynb#X51sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(train_output,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documentos/Doutorado/Projeto/Criatividade/Projetos/humor-recognition-knowledge-injection/scripts/notebooks/multimodal_classification.ipynb#X51sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m                                   save_strategy\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documentos/Doutorado/Projeto/Criatividade/Projetos/humor-recognition-knowledge-injection/scripts/notebooks/multimodal_classification.ipynb#X51sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m                                   save_total_limit\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documentos/Doutorado/Projeto/Criatividade/Projetos/humor-recognition-knowledge-injection/scripts/notebooks/multimodal_classification.ipynb#X51sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m                                   learning_rate\u001b[39m=\u001b[39m\u001b[39m5e-5\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documentos/Doutorado/Projeto/Criatividade/Projetos/humor-recognition-knowledge-injection/scripts/notebooks/multimodal_classification.ipynb#X51sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m                                   num_train_epochs\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documentos/Doutorado/Projeto/Criatividade/Projetos/humor-recognition-knowledge-injection/scripts/notebooks/multimodal_classification.ipynb#X51sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(model, training_args,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documentos/Doutorado/Projeto/Criatividade/Projetos/humor-recognition-knowledge-injection/scripts/notebooks/multimodal_classification.ipynb#X51sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m                   train_dataset\u001b[39m=\u001b[39mtokenized_data,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documentos/Doutorado/Projeto/Criatividade/Projetos/humor-recognition-knowledge-injection/scripts/notebooks/multimodal_classification.ipynb#X51sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m                   data_collator\u001b[39m=\u001b[39mdata_collator,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documentos/Doutorado/Projeto/Criatividade/Projetos/humor-recognition-knowledge-injection/scripts/notebooks/multimodal_classification.ipynb#X51sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m                   tokenizer\u001b[39m=\u001b[39mtokenizer)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Documentos/Doutorado/Projeto/Criatividade/Projetos/humor-recognition-knowledge-injection/scripts/notebooks/multimodal_classification.ipynb#X51sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documentos/Doutorado/Projeto/Criatividade/Projetos/humor-recognition-knowledge-injection/scripts/notebooks/multimodal_classification.ipynb#X51sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m###### PRECITION ######\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documentos/Doutorado/Projeto/Criatividade/Projetos/humor-recognition-knowledge-injection/scripts/notebooks/multimodal_classification.ipynb#X51sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m test_corpus \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_json(test_corpus_path)\n",
      "File \u001b[1;32md:\\.virtualenvs\\humor-recognition-knowledge-injection-zyMLYgwE\\lib\\site-packages\\transformers\\trainer.py:1591\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1589\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1590\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1591\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1592\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1593\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1594\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1595\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1596\u001b[0m     )\n",
      "File \u001b[1;32md:\\.virtualenvs\\humor-recognition-knowledge-injection-zyMLYgwE\\lib\\site-packages\\transformers\\trainer.py:1870\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1867\u001b[0m     rng_to_sync \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m-> 1870\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   1871\u001b[0m     total_batched_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1872\u001b[0m     \u001b[39mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[1;32md:\\.virtualenvs\\humor-recognition-knowledge-injection-zyMLYgwE\\lib\\site-packages\\accelerate\\data_loader.py:460\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[39m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[39;00m\n\u001b[0;32m    459\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 460\u001b[0m         current_batch \u001b[39m=\u001b[39m send_to_device(current_batch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[0;32m    461\u001b[0m     next_batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(dataloader_iter)\n\u001b[0;32m    462\u001b[0m     \u001b[39mif\u001b[39;00m batch_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskip_batches:\n",
      "File \u001b[1;32md:\\.virtualenvs\\humor-recognition-knowledge-injection-zyMLYgwE\\lib\\site-packages\\accelerate\\utils\\operations.py:160\u001b[0m, in \u001b[0;36msend_to_device\u001b[1;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[39melif\u001b[39;00m skip_keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m         skip_keys \u001b[39m=\u001b[39m []\n\u001b[0;32m    159\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(tensor)(\n\u001b[1;32m--> 160\u001b[0m         {\n\u001b[0;32m    161\u001b[0m             k: t \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m skip_keys \u001b[39melse\u001b[39;00m send_to_device(t, device, non_blocking\u001b[39m=\u001b[39mnon_blocking, skip_keys\u001b[39m=\u001b[39mskip_keys)\n\u001b[0;32m    162\u001b[0m             \u001b[39mfor\u001b[39;00m k, t \u001b[39min\u001b[39;00m tensor\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    163\u001b[0m         }\n\u001b[0;32m    164\u001b[0m     )\n\u001b[0;32m    165\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(tensor, \u001b[39m\"\u001b[39m\u001b[39mto\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    166\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\.virtualenvs\\humor-recognition-knowledge-injection-zyMLYgwE\\lib\\site-packages\\accelerate\\utils\\operations.py:161\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[39melif\u001b[39;00m skip_keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m         skip_keys \u001b[39m=\u001b[39m []\n\u001b[0;32m    159\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(tensor)(\n\u001b[0;32m    160\u001b[0m         {\n\u001b[1;32m--> 161\u001b[0m             k: t \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m skip_keys \u001b[39melse\u001b[39;00m send_to_device(t, device, non_blocking\u001b[39m=\u001b[39;49mnon_blocking, skip_keys\u001b[39m=\u001b[39;49mskip_keys)\n\u001b[0;32m    162\u001b[0m             \u001b[39mfor\u001b[39;00m k, t \u001b[39min\u001b[39;00m tensor\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    163\u001b[0m         }\n\u001b[0;32m    164\u001b[0m     )\n\u001b[0;32m    165\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(tensor, \u001b[39m\"\u001b[39m\u001b[39mto\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    166\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\.virtualenvs\\humor-recognition-knowledge-injection-zyMLYgwE\\lib\\site-packages\\accelerate\\utils\\operations.py:167\u001b[0m, in \u001b[0;36msend_to_device\u001b[1;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(tensor, \u001b[39m\"\u001b[39m\u001b[39mto\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    166\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 167\u001b[0m         \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49mto(device, non_blocking\u001b[39m=\u001b[39;49mnon_blocking)\n\u001b[0;32m    168\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:  \u001b[39m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[0;32m    169\u001b[0m         \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_output = Path('results/models/bertimbau_large/fold_0')\n",
    "test_output = Path('results/predictions/bertimbau_large/fold_0.csv')\n",
    "\n",
    "###### TRAINING ######\n",
    "train_corpus = pd.read_json(train_corpus_path)\n",
    "labels = train_corpus['Label'].unique()\n",
    "num_labels = len(labels)\n",
    "label2id = {label: id_ for id_, label in enumerate(labels)}\n",
    "id2label = {label2id[label]: label for label in label2id}\n",
    "train_corpus['Label'] = train_corpus['Label'].map(label2id)\n",
    "\n",
    "data = Dataset.from_pandas(train_corpus[['Text', 'Label']])\n",
    "data = data.rename_column('Label', 'label')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['Text'])\n",
    "\n",
    "\n",
    "tokenized_data = data.map(tokenize_function, batched=True)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint,\n",
    "                                                           num_labels=num_labels,\n",
    "                                                           label2id=label2id,\n",
    "                                                           id2label=id2label)\n",
    "\n",
    "training_args = TrainingArguments(train_output,\n",
    "                                  save_strategy='epoch',\n",
    "                                  save_total_limit=1,\n",
    "                                  learning_rate=5e-5,\n",
    "                                  num_train_epochs=5)\n",
    "trainer = Trainer(model, training_args,\n",
    "                  train_dataset=tokenized_data,\n",
    "                  data_collator=data_collator,\n",
    "                  tokenizer=tokenizer)\n",
    "trainer.train()\n",
    "\n",
    "###### PRECITION ######\n",
    "test_corpus = pd.read_json(test_corpus_path)\n",
    "test_data = test_corpus['Text'].to_list()\n",
    "\n",
    "text_classification = pipeline('text-classification',\n",
    "                               model=model,\n",
    "                               tokenizer=tokenizer,\n",
    "                               device=0)\n",
    "predictions = text_classification(test_data)\n",
    "\n",
    "results = pd.DataFrame(predictions, index=test_corpus.index)\n",
    "results = results.drop(columns='score')\n",
    "results = results.rename(columns={'label': 'Prediction'})\n",
    "results['Label'] = test_corpus['Label']\n",
    "\n",
    "test_output.parent.mkdir(parents=True, exist_ok=True)\n",
    "results.to_csv(test_output, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Albertina PT-BR Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'C:/Users/Márcio Lima/.cache/huggingface/hub/models--PORTULAN--albertina-ptbr-base/snapshots/b6fb59d5f833001988d393a0137c64b4ec641777'\n",
    "checkpoint_type = 'deberta'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base model: $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d868afe8c94037aeaab0678c0c0b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8825 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at C:/Users/Márcio Lima/.cache/huggingface/hub/models--PORTULAN--albertina-ptbr-base/snapshots/b6fb59d5f833001988d393a0137c64b4ec641777 and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abf508c135224682bdccd93520d39fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5520 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7024, 'learning_rate': 4.547101449275363e-05, 'epoch': 0.45}\n",
      "{'loss': 0.6947, 'learning_rate': 4.094202898550725e-05, 'epoch': 0.91}\n",
      "{'loss': 0.6949, 'learning_rate': 3.641304347826087e-05, 'epoch': 1.36}\n",
      "{'loss': 0.6959, 'learning_rate': 3.188405797101449e-05, 'epoch': 1.81}\n",
      "{'loss': 0.695, 'learning_rate': 2.7355072463768118e-05, 'epoch': 2.26}\n",
      "{'loss': 0.6938, 'learning_rate': 2.282608695652174e-05, 'epoch': 2.72}\n",
      "{'loss': 0.6939, 'learning_rate': 1.8297101449275363e-05, 'epoch': 3.17}\n",
      "{'loss': 0.6945, 'learning_rate': 1.3768115942028985e-05, 'epoch': 3.62}\n",
      "{'loss': 0.6937, 'learning_rate': 9.239130434782608e-06, 'epoch': 4.08}\n",
      "{'loss': 0.694, 'learning_rate': 4.710144927536232e-06, 'epoch': 4.53}\n",
      "{'loss': 0.6934, 'learning_rate': 1.8115942028985507e-07, 'epoch': 4.98}\n",
      "{'train_runtime': 830.6053, 'train_samples_per_second': 53.124, 'train_steps_per_second': 6.646, 'train_loss': 0.6951023893079896, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "train_output = Path('results/models/albertina_ptbr_base/fold_0')\n",
    "test_output = Path('results/predictions/albertina_ptbr_base/fold_0.csv')\n",
    "\n",
    "###### TRAINING ######\n",
    "train_corpus = pd.read_json(train_corpus_path)\n",
    "labels = train_corpus['Label'].unique()\n",
    "num_labels = len(labels)\n",
    "label2id = {label: id_ for id_, label in enumerate(labels)}\n",
    "id2label = {label2id[label]: label for label in label2id}\n",
    "train_corpus['Label'] = train_corpus['Label'].map(label2id)\n",
    "\n",
    "data = Dataset.from_pandas(train_corpus[['Text', 'Label']])\n",
    "data = data.rename_column('Label', 'label')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['Text'])\n",
    "\n",
    "\n",
    "tokenized_data = data.map(tokenize_function, batched=True)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint,\n",
    "                                                           num_labels=num_labels,\n",
    "                                                           label2id=label2id,\n",
    "                                                           id2label=id2label)\n",
    "\n",
    "training_args = TrainingArguments(train_output,\n",
    "                                  save_strategy='epoch',\n",
    "                                  save_total_limit=1,\n",
    "                                  learning_rate=5e-5,\n",
    "                                  num_train_epochs=5)\n",
    "trainer = Trainer(model, training_args,\n",
    "                  train_dataset=tokenized_data,\n",
    "                  data_collator=data_collator,\n",
    "                  tokenizer=tokenizer)\n",
    "trainer.train()\n",
    "\n",
    "###### PRECITION ######\n",
    "test_corpus = pd.read_json(test_corpus_path)\n",
    "test_data = test_corpus['Text'].to_list()\n",
    "\n",
    "text_classification = pipeline('text-classification',\n",
    "                               model=model,\n",
    "                               tokenizer=tokenizer,\n",
    "                               device=0)\n",
    "predictions = text_classification(test_data)\n",
    "\n",
    "results = pd.DataFrame(predictions, index=test_corpus.index)\n",
    "results = results.drop(columns='score')\n",
    "results = results.rename(columns={'label': 'Prediction'})\n",
    "results['Label'] = test_corpus['Label']\n",
    "\n",
    "test_output.parent.mkdir(parents=True, exist_ok=True)\n",
    "results.to_csv(test_output, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation: $x\\|n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c4dfdd0ebd43e5ae2e327c4b150c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7058, 'learning_rate': 4.094202898550725e-05, 'epoch': 0.91}\n",
      "{'loss': 0.695, 'learning_rate': 3.188405797101449e-05, 'epoch': 1.81}\n",
      "{'loss': 0.6935, 'learning_rate': 2.282608695652174e-05, 'epoch': 2.72}\n",
      "{'loss': 0.6933, 'learning_rate': 1.3768115942028985e-05, 'epoch': 3.62}\n",
      "{'loss': 0.6925, 'learning_rate': 4.710144927536232e-06, 'epoch': 4.53}\n",
      "{'train_runtime': 752.2847, 'train_samples_per_second': 58.655, 'train_steps_per_second': 3.669, 'train_loss': 0.6955817733985791, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "train_output = Path('results/models/albertina_ptbr_base_concatenation/fold_0')\n",
    "test_output = Path('results/predictions/albertina_ptbr_base_concatenation/fold_0.csv')\n",
    "\n",
    "trained_model = train(train_corpus_path,\n",
    "                      train_features_path,\n",
    "                      checkpoint,\n",
    "                      checkpoint_type,\n",
    "                      ClassificationModelConcatenation,\n",
    "                      train_output)\n",
    "predict(test_corpus_path,\n",
    "        test_features_path,\n",
    "        trained_model,\n",
    "        test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features pooling: $x\\|\\mathrm{MLP}(n)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f2353dc20a4003b2f4b4f04652ac9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7013, 'learning_rate': 4.094202898550725e-05, 'epoch': 0.91}\n",
      "{'loss': 0.6956, 'learning_rate': 3.188405797101449e-05, 'epoch': 1.81}\n",
      "{'loss': 0.6946, 'learning_rate': 2.282608695652174e-05, 'epoch': 2.72}\n",
      "{'loss': 0.6939, 'learning_rate': 1.3768115942028985e-05, 'epoch': 3.62}\n",
      "{'loss': 0.6932, 'learning_rate': 4.710144927536232e-06, 'epoch': 4.53}\n",
      "{'train_runtime': 992.6211, 'train_samples_per_second': 44.453, 'train_steps_per_second': 2.781, 'train_loss': 0.6955286882925724, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "train_output = Path('results/models/albertina_ptbr_base_features_pooling/fold_0')\n",
    "test_output = Path('results/predictions/albertina_ptbr_base_features_pooling/fold_0.csv')\n",
    "\n",
    "trained_model = train(train_corpus_path,\n",
    "                      train_features_path,\n",
    "                      checkpoint,\n",
    "                      checkpoint_type,\n",
    "                      ClassificationModelFeaturesPooling,\n",
    "                      train_output)\n",
    "predict(test_corpus_path,\n",
    "        test_features_path,\n",
    "        trained_model,\n",
    "        test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared representation: $\\mathrm{MLP}(x\\|\\mathrm{MLP(n)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85f1b35a100148b3a1d409911ec06670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6941, 'learning_rate': 4.094202898550725e-05, 'epoch': 0.91}\n",
      "{'loss': 0.6926, 'learning_rate': 3.188405797101449e-05, 'epoch': 1.81}\n",
      "{'loss': 0.6913, 'learning_rate': 2.282608695652174e-05, 'epoch': 2.72}\n",
      "{'loss': 0.6911, 'learning_rate': 1.3768115942028985e-05, 'epoch': 3.62}\n",
      "{'loss': 0.6901, 'learning_rate': 4.710144927536232e-06, 'epoch': 4.53}\n",
      "{'train_runtime': 970.4675, 'train_samples_per_second': 45.468, 'train_steps_per_second': 2.844, 'train_loss': 0.6917266126992045, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "train_output = Path('results/models/albertina_ptbr_base_shared_representation/fold_0')\n",
    "test_output = Path('results/predictions/albertina_ptbr_base_shared_representation/fold_0.csv')\n",
    "\n",
    "trained_model = train(train_corpus_path,\n",
    "                      train_features_path,\n",
    "                      checkpoint,\n",
    "                      checkpoint_type,\n",
    "                      ClassificationModelSharedRepresentation,\n",
    "                      train_output)\n",
    "predict(test_corpus_path,\n",
    "        test_features_path,\n",
    "        trained_model,\n",
    "        test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Albertina PT-PT Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'C:/Users/Márcio Lima/.cache/huggingface/hub/models--PORTULAN--albertina-ptpt-base/snapshots/b0a8b33132b56c09e6a67fbc24db8c655cae8f76'\n",
    "checkpoint_type = 'deberta'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base model: $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bdf21b72653488ba84c29263a758671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8825 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at C:/Users/Márcio Lima/.cache/huggingface/hub/models--PORTULAN--albertina-ptpt-base/snapshots/b0a8b33132b56c09e6a67fbc24db8c655cae8f76 and are newly initialized: ['classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f86cdc43684fa0830bc9cbd99a0825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5520 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.702, 'learning_rate': 4.547101449275363e-05, 'epoch': 0.45}\n",
      "{'loss': 0.6968, 'learning_rate': 4.094202898550725e-05, 'epoch': 0.91}\n",
      "{'loss': 0.6973, 'learning_rate': 3.641304347826087e-05, 'epoch': 1.36}\n",
      "{'loss': 0.697, 'learning_rate': 3.188405797101449e-05, 'epoch': 1.81}\n",
      "{'loss': 0.6956, 'learning_rate': 2.7355072463768118e-05, 'epoch': 2.26}\n",
      "{'loss': 0.6949, 'learning_rate': 2.282608695652174e-05, 'epoch': 2.72}\n",
      "{'loss': 0.6947, 'learning_rate': 1.8297101449275363e-05, 'epoch': 3.17}\n",
      "{'loss': 0.695, 'learning_rate': 1.3768115942028985e-05, 'epoch': 3.62}\n",
      "{'loss': 0.6945, 'learning_rate': 9.239130434782608e-06, 'epoch': 4.08}\n",
      "{'loss': 0.6941, 'learning_rate': 4.710144927536232e-06, 'epoch': 4.53}\n",
      "{'loss': 0.6936, 'learning_rate': 1.8115942028985507e-07, 'epoch': 4.98}\n",
      "{'train_runtime': 838.9497, 'train_samples_per_second': 52.596, 'train_steps_per_second': 6.58, 'train_loss': 0.6959295789400737, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "train_output = Path('results/models/albertina_ptpt_base/fold_0')\n",
    "test_output = Path('results/predictions/albertina_ptpt_base/fold_0.csv')\n",
    "\n",
    "###### TRAINING ######\n",
    "train_corpus = pd.read_json(train_corpus_path)\n",
    "labels = train_corpus['Label'].unique()\n",
    "num_labels = len(labels)\n",
    "label2id = {label: id_ for id_, label in enumerate(labels)}\n",
    "id2label = {label2id[label]: label for label in label2id}\n",
    "train_corpus['Label'] = train_corpus['Label'].map(label2id)\n",
    "\n",
    "data = Dataset.from_pandas(train_corpus[['Text', 'Label']])\n",
    "data = data.rename_column('Label', 'label')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['Text'])\n",
    "\n",
    "\n",
    "tokenized_data = data.map(tokenize_function, batched=True)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint,\n",
    "                                                           num_labels=num_labels,\n",
    "                                                           label2id=label2id,\n",
    "                                                           id2label=id2label)\n",
    "\n",
    "training_args = TrainingArguments(train_output,\n",
    "                                  save_strategy='epoch',\n",
    "                                  save_total_limit=1,\n",
    "                                  learning_rate=5e-5,\n",
    "                                  num_train_epochs=5)\n",
    "trainer = Trainer(model, training_args,\n",
    "                  train_dataset=tokenized_data,\n",
    "                  data_collator=data_collator,\n",
    "                  tokenizer=tokenizer)\n",
    "trainer.train()\n",
    "\n",
    "###### PRECITION ######\n",
    "test_corpus = pd.read_json(test_corpus_path)\n",
    "test_data = test_corpus['Text'].to_list()\n",
    "\n",
    "text_classification = pipeline('text-classification',\n",
    "                               model=model,\n",
    "                               tokenizer=tokenizer,\n",
    "                               device=0)\n",
    "predictions = text_classification(test_data)\n",
    "\n",
    "results = pd.DataFrame(predictions, index=test_corpus.index)\n",
    "results = results.drop(columns='score')\n",
    "results = results.rename(columns={'label': 'Prediction'})\n",
    "results['Label'] = test_corpus['Label']\n",
    "\n",
    "test_output.parent.mkdir(parents=True, exist_ok=True)\n",
    "results.to_csv(test_output, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation: $x\\|n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baac05da6808464095e2addde2798123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5879, 'learning_rate': 4.094202898550725e-05, 'epoch': 0.91}\n",
      "{'loss': 0.4554, 'learning_rate': 3.188405797101449e-05, 'epoch': 1.81}\n",
      "{'loss': 0.3538, 'learning_rate': 2.282608695652174e-05, 'epoch': 2.72}\n",
      "{'loss': 0.2672, 'learning_rate': 1.3768115942028985e-05, 'epoch': 3.62}\n",
      "{'loss': 0.2058, 'learning_rate': 4.710144927536232e-06, 'epoch': 4.53}\n",
      "{'train_runtime': 744.3634, 'train_samples_per_second': 59.279, 'train_steps_per_second': 3.708, 'train_loss': 0.35547824389692667, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "train_output = Path('results/models/albertina_ptpt_base_concatenation/fold_0')\n",
    "test_output = Path('results/predictions/albertina_ptpt_base_concatenation/fold_0.csv')\n",
    "\n",
    "trained_model = train(train_corpus_path,\n",
    "                      train_features_path,\n",
    "                      checkpoint,\n",
    "                      checkpoint_type,\n",
    "                      ClassificationModelConcatenation,\n",
    "                      train_output)\n",
    "predict(test_corpus_path,\n",
    "        test_features_path,\n",
    "        trained_model,\n",
    "        test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features pooling: $x\\|\\mathrm{MLP}(n)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c913a5e4b434b9996112079cecafba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5881, 'learning_rate': 4.094202898550725e-05, 'epoch': 0.91}\n",
      "{'loss': 0.4583, 'learning_rate': 3.188405797101449e-05, 'epoch': 1.81}\n",
      "{'loss': 0.3607, 'learning_rate': 2.282608695652174e-05, 'epoch': 2.72}\n",
      "{'loss': 0.2814, 'learning_rate': 1.3768115942028985e-05, 'epoch': 3.62}\n",
      "{'loss': 0.217, 'learning_rate': 4.710144927536232e-06, 'epoch': 4.53}\n",
      "{'train_runtime': 740.2392, 'train_samples_per_second': 59.609, 'train_steps_per_second': 3.729, 'train_loss': 0.3624687775321629, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "train_output = Path('results/models/albertina_ptpt_base_features_pooling/fold_0')\n",
    "test_output = Path('results/predictions/albertina_ptpt_base_features_pooling/fold_0.csv')\n",
    "\n",
    "trained_model = train(train_corpus_path,\n",
    "                      train_features_path,\n",
    "                      checkpoint,\n",
    "                      checkpoint_type,\n",
    "                      ClassificationModelFeaturesPooling,\n",
    "                      train_output)\n",
    "predict(test_corpus_path,\n",
    "        test_features_path,\n",
    "        trained_model,\n",
    "        test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared representation: $\\mathrm{MLP}(x\\|\\mathrm{MLP(n)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d55abe2b4a44612975a35e471e89751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6731, 'learning_rate': 4.094202898550725e-05, 'epoch': 0.91}\n",
      "{'loss': 0.6503, 'learning_rate': 3.188405797101449e-05, 'epoch': 1.81}\n",
      "{'loss': 0.6462, 'learning_rate': 2.282608695652174e-05, 'epoch': 2.72}\n",
      "{'loss': 0.6426, 'learning_rate': 1.3768115942028985e-05, 'epoch': 3.62}\n",
      "{'loss': 0.6375, 'learning_rate': 4.710144927536232e-06, 'epoch': 4.53}\n",
      "{'train_runtime': 749.9174, 'train_samples_per_second': 58.84, 'train_steps_per_second': 3.68, 'train_loss': 0.6492790719737177, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "train_output = Path('results/models/albertina_ptpt_base_shared_representation/fold_0')\n",
    "test_output = Path('results/predictions/albertina_ptpt_base_shared_representation/fold_0.csv')\n",
    "\n",
    "trained_model = train(train_corpus_path,\n",
    "                      train_features_path,\n",
    "                      checkpoint,\n",
    "                      checkpoint_type,\n",
    "                      ClassificationModelSharedRepresentation,\n",
    "                      train_output)\n",
    "predict(test_corpus_path,\n",
    "        test_features_path,\n",
    "        trained_model,\n",
    "        test_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "humor-recognition-knowledge-injection-zyMLYgwE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
